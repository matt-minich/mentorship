{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "301de9ef",
   "metadata": {},
   "source": [
    "# Pulling and formatting timing files\n",
    "\n",
    "Once you've finished preprocessing, you'll need to pull the timing files for all of the participants you're interested in running analysis on. This process will pull data from the participants' log files and create time series for each condition of interest. \n",
    "\n",
    "The code presented here can be edited (or run) in the terminal by accessing two scripts:\n",
    "1. B3/scripts/step_3_a_pullingtimings.py\n",
    "2. B3/scripts/step_3_b_formattingtimings.sh\n",
    "\n",
    "The first script should pull all of the major conditions for the self-other, peer feedback, and conformity tasks... but it's not yet designed to handle timings for cyberball (which only exports a raw long and will need to be handled differently). I've tried to identify all of the relevant categories of stimuli here, but you will likely want to edit this script as you develop new research questions. \n",
    "\n",
    "The second script will adapt all of the timings to the .1D format preferred by AFNI and will stitch together timings from two-run tasks into a single file. \n",
    "\n",
    "### Prerequisite steps: \n",
    "By the time you get here, you should have: \n",
    "* Completed preprocessing for all of the subjects of interest (see the notebooks for STEP_1 and STEP_2)\n",
    "* Gained access to the scan logs on Box \n",
    "* Mapped the DOP-Restricted drive to your computer\n",
    "\n",
    "#### Note: \n",
    "This notebook documents the process of pulling timings that I designed for AFNI analysis. That process works, but it was not designed with the BIDS format in mind. To obtain BIDS-friendly timing files (which may be needed for future analyses that use nipype), refer to BIDS_timings_maker.ipynb (a script that should do this for the first three tasks). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8aa73f",
   "metadata": {},
   "source": [
    "## Step 1: Downloading and copying the timings\n",
    "\n",
    "First, you'll need to get the raw log files for the participants you're interested in. Taking care to choose only the logs for the correct scans (check the scan notes), download all of the log files and move them to the proper directory in B3/timings. Log files are formatted like such:\n",
    "* Self-other run 1 (participant 135): 135_task1_a.csv  -- should go in B3/logs/self_other_all\n",
    "* Self-other run 2 (participant 135): 135_task1_b.csv -- should go in B3/logs/self_other_all\n",
    "* Peer feedback run 1 (participant 135): 135_task2_a.csv -- should go in B3/logs/peer_feedback_all\n",
    "* ...and so on. You get the idea.\n",
    "\n",
    "For those initial participants who only had one scan, you'll need to rename the log files. Use a similar format as the above so that:\n",
    "* Self-other (participant 094): 94_task1_socialmedia.csv = 094_task1_ab.csv\n",
    "* Conformity (participant 094): 94_task3_social-influence.csv = 094_task3_ab.csv\n",
    "\n",
    "As you might have noticed above, you'll also need to edit some participant IDs to make them three digits long. Just add leading zeros so that 8 = 008, etc. If the file for the correct scan has an extra number at the end (for example task3_a_1.csv), remove that number (to mate it task3_a.csv, for example). \n",
    "\n",
    "#### Note: \n",
    "This process is tedious, and it would be great if someone could automate it. The only caveat for whoever DOES automate it is to MAKE SURE that the script is not blindly pulling all task1_a and task1_b files for each participant. In many cases, the correct file is task1_a_1.csv or even task1_a_3.csv because of errors during the scan. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ac22b",
   "metadata": {},
   "source": [
    "## Step 2: Pulling timings\n",
    "\n",
    "Next, you'll just run the script B3/scripts/step_three_a_pullingtimings.py. That script contains functions that pull timings for the self-other, peer feedback, and conformity tasks... but nothing yet for Cyberball. Because that script is long and will likely need some tinkering as analysis needs change, I've broken it out section by section in the cells below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3bbb56",
   "metadata": {},
   "source": [
    "### Loading the libraries and setting up the button box dictionaries\n",
    "\n",
    "The first few lines of this script just load in the libraries we'll need (glob, os, numpy, and pandas), then create dictionaries for a few button box responses codes:\n",
    "* button_keys just assigns the correct number to the color of button pressed in the button box. If things look backwards in analysis, triple check this to make sure it isn't coded backwards (I've checked, but somehow never feel 100% sure due to past analysis traumas). \n",
    "* button_keys_inv just reverses the codes, so that the highest rating is coded 1 and the lowest rating is coded 4. \n",
    "* button_keys_exp codes the button box responses as if they represent an exponential growth function (each number is e^rating). This is part of a fishing expedition I'm on to explain something about responses peer feedback ratings. The numbers may be changed in the future, because I suspect some modifier other than e needs to be added to flatten the curve a little.  \n",
    "* button_keys_exp_inv is just the inverse of the growth function... so it represents exponential decay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5886a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "button_keys = {'b':1,'y':2,'g':3,'r':4} # This encodes the button box responses. THERE IS ALWAYS A CHANCE THIS MIGHT BE BACKWARDS.\n",
    "button_keys_inv = {'b':4,'y':3,'g':2,'r':1}\n",
    "button_keys_exp = {'b':2.718, 'y':7.389, 'g':20.086, 'r': 58.594} # This encodes the responeses and transforms them for an exponential relationship. b = e^1, r = e^4\n",
    "button_keys_exp_inv = {'b': 58.594, 'y': 20.086, 'g': 7.389, 'r': 2.718}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d392aad7",
   "metadata": {},
   "source": [
    "### Pulling the self-other timings\n",
    "\n",
    "This function pulls all of the self-other timing files from a given .csv. Most of the names here should be obvious to anyone familiar with the task, but here are few descriptions:\n",
    "* \"other_file\" is a file containing the timings of all other-generated stimuli\n",
    "* \"self_file\" is a file containing the timings of all self-generated simuli\n",
    "* \"like_file\" is a file containing the timings of all stimuli the participant rated 3 or 4\n",
    "* \"certain_file\" is a file for which the participant gave a certainty rating of 3 or 4\n",
    "* ... and so on\n",
    "\n",
    "A few other files of interest are\n",
    "* \"fixation_file\" is a file that stores both the timing and the duration of every display of the fixation cross.\n",
    "* \"rating_pmod_file\" is a file that stores the timing of each stimuli, paired with the participant's rating of that stimuli.\n",
    "* \"certainty_pmod_file\" is a file that stores the timing of each stimuli paired with the participant's certainty rating of that stimuli. This allows us to test whether BOLD signal varies as a linear function of ratings.  \n",
    "* \"rating_pmod_exp_file\" is a file that stores the timing of each stimuli paired with e^rating. This allows us to test whether BOLD signal varies as an exponential growth function of ratings. \n",
    "* ... and so on.\n",
    "\n",
    "This function basically just opens a bunch of different files, the uses a bunch of \"with\" statements to write those files based on the content of certain cells in the .csv. Knowing that, you should be able to build timings for whatever condition you want. Just add a file and a new \"with\" statement containing the logic. \n",
    "\n",
    "You'll notice that all of the timings are the logged moment of stimuli presentation started MINUS \"value\". This value represents the number of seconds that went by before the scanner was actually started, plus seven seconds to account for the seven seconds we trimmed off the front of each file in preprocessing. The if/elif statement that creates this value is there because the name of that column in our log files changed at some point after scans started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "283536d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selfother_times(file_path, run_id):\n",
    "    df = pd.read_csv(file_path)\n",
    "    other_file = f\"../timings/{run_id}_selfother_other.txt\"\n",
    "    self_file = f\"../timings/{run_id}_selfother_self.txt\"\n",
    "    like_file = f\"../timings/{run_id}_selfother_like.txt\"\n",
    "    dislike_file = f\"../timings/{run_id}_selfother_dislike.txt\"\n",
    "    self_like_file = f\"../timings/{run_id}_selfother_selflike.txt\"\n",
    "    self_dislike_file = f\"../timings/{run_id}_selfother_selfdislike.txt\"\n",
    "    other_like_file = f\"../timings/{run_id}_selfother_otherlike.txt\"\n",
    "    other_dislike_file = f\"../timings/{run_id}_selfother_otherdislike.txt\"\n",
    "    certain_file = f\"../timings/{run_id}_selfother_certain.txt\"\n",
    "    uncertain_file = f\"../timings/{run_id}_selfother_uncertain.txt\"\n",
    "    fixation_file = f\"../timings/{run_id}_selfother_fixation.txt\"\n",
    "    certainty_pmod_file = f\"../timings/{run_id}_selfother_certainty_pmod.txt\"\n",
    "    rating_pmod_file = f\"../timings/{run_id}_selfother_rating_pmod.txt\"\n",
    "    rating_pmod_inv_file = f\"../timings/{run_id}_selfother_rating_pmod_inv.txt\"\n",
    "    rating_pmod_exp_file = f\"../timings/{run_id}_selfother_rating_pmod_exp.txt\"\n",
    "    rating_pmod_exp_inv_file = f\"../timings/{run_id}_selfother_rating_pmod_exp_inv.txt\"\n",
    "    if 'sm_directions.stopped' in df.columns:\n",
    "        value = df.loc[1,'sm_directions.started'] + 7\n",
    "    elif 'sm_instructions.stopped' in df.columns:\n",
    "        value = df.loc[1,'sm_instructions.started'] + 7\n",
    "    print(f'reading {file_path}')\n",
    "    with open(fixation_file, 'w')  as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "                timing = row['fixation_cross.started']-value\n",
    "                duration = row['fixation_duration']\n",
    "                file.write(f'{timing} {duration} \\n' )\n",
    "    with open(other_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['type'] == 'peer':\n",
    "                timing = row['socialmedia_stim.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(self_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['type'] == 'self':\n",
    "                timing = row['socialmedia_stim.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(like_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_rateresp_box.keys'] == 'r' or row['sm_rateresp_box.keys'] == 'g':\n",
    "                timing = row['socialmedia_stim.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(dislike_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_rateresp_box.keys'] == 'b' or row['sm_rateresp_box.keys'] == 'y':\n",
    "                timing = row['socialmedia_stim.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(self_like_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['type'] == 'self' and (row['sm_rateresp_box.keys'] == 'r' or row['sm_rateresp_box.keys'] == 'g'):\n",
    "                timing = row['socialmedia_stim.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(self_dislike_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['type'] == 'self' and (row['sm_rateresp_box.keys'] == 'b' or row['sm_rateresp_box.keys'] == 'y'):\n",
    "                timing = row['socialmedia_stim.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(other_like_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['type'] == 'peer' and (row['sm_rateresp_box.keys'] == 'r' or row['sm_rateresp_box.keys'] == 'g'):\n",
    "                timing = row['socialmedia_stim.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(other_dislike_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['type'] == 'peer' and (row['sm_rateresp_box.keys'] == 'b' or row['sm_rateresp_box.keys'] == 'y'):\n",
    "                timing = row['socialmedia_stim.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(certain_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "           if row['certainty_box.keys'] == 'r' or row['certainty_box.keys'] == 'g':\n",
    "               timing = row['socialmedia_stim.started']-value\n",
    "               file.write(f\"{timing} \\n\")\n",
    "    with open(uncertain_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "           if row['certainty_box.keys'] == 'b' or row['certainty_box.keys'] == 'y':\n",
    "               timing = row['socialmedia_stim.started']-value\n",
    "               file.write(f\"{timing} \\n\")\n",
    "    with open(certainty_pmod_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['certainty_box.keys'] in button_keys:\n",
    "                resp = button_keys.get(row['certainty_box.keys'], None)\n",
    "                timing = row['socialmedia_stim.started']-value\n",
    "                file.write(f\"{timing}  5  {resp} \\n\")\n",
    "    with open(rating_pmod_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_rateresp_box.keys'] in button_keys:\n",
    "                resp = button_keys.get(row['sm_rateresp_box.keys'], None)\n",
    "                timing = row['socialmedia_stim.started']-value\n",
    "                file.write(f\"{timing}  5  {resp} \\n\")\n",
    "    with open(rating_pmod_inv_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_rateresp_box.keys'] in button_keys:\n",
    "                resp = button_keys_inv.get(row['sm_rateresp_box.keys'], None)\n",
    "                timing = row['socialmedia_stim.started']-value\n",
    "                file.write(f\"{timing}  5  {resp} \\n\")\n",
    "    with open(rating_pmod_exp_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_rateresp_box.keys'] in button_keys :\n",
    "                resp = button_keys_exp.get(row['sm_rateresp_box.keys'], None)\n",
    "                timing = row['socialmedia_stim.started']-value\n",
    "                file.write(f\"{timing}  5  {resp} \\n\")\n",
    "    with open(rating_pmod_exp_inv_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_rateresp_box.keys'] in button_keys:\n",
    "                resp = button_keys_exp_inv.get(row['sm_rateresp_box.keys'], None)\n",
    "                timing = row['socialmedia_stim.started']-value\n",
    "                file.write(f\"{timing}  5  {resp} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8cc13b",
   "metadata": {},
   "source": [
    "### Pulling the peer feedback timings\n",
    "\n",
    "This function does the same thing for peer feedback timings. I'll spare you the details of each file, but some important notes are:\n",
    "* \"peer_like\" and \"peer_dislike\" refer to the feedback given by \"peers\" (peer_like is a rating of 3 or 4)\n",
    "* \"good\" and \"bad\" refer to participant responses to the prompt \"How did this feedback make you feel?\" (good is 3 or 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92af34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peerfeedback_times(file_path, run_id):\n",
    "    df = pd.read_csv(file_path)\n",
    "    green_file = f\"../timings/{run_id}_peerfeedback_green.txt\"\n",
    "    yellow_file = f\"../timings/{run_id}_peerfeedback_yellow.txt\"\n",
    "    orange_file = f\"../timings/{run_id}_peerfeedback_orange.txt\"\n",
    "    peer_like_file = f\"../timings/{run_id}_peerfeedback_peer_like.txt\"\n",
    "    peer_dislike_file = f\"../timings/{run_id}_peerfeedback_peer_dislike.txt\"\n",
    "    good_file = f\"../timings/{run_id}_peerfeedback_good.txt\"\n",
    "    bad_file = f\"../timings/{run_id}_peerfeedback_bad.txt\"\n",
    "    green_like_file = f\"../timings/{run_id}_peerfeedback_green_like.txt\"\n",
    "    green_dislike_file = f\"../timings/{run_id}_peerfeedback_green_dislike.txt\"\n",
    "    yellow_like_file = f\"../timings/{run_id}_peerfeedback_yellow_like.txt\"\n",
    "    yellow_dislike_file = f\"../timings/{run_id}_peerfeedback_yellow_dislike.txt\"\n",
    "    orange_like_file = f\"../timings/{run_id}_peerfeedback_orange_like.txt\"\n",
    "    orange_dislike_file = f\"../timings/{run_id}_peerfeedback_orange_dislike.txt\"\n",
    "    fixation_file = f\"../timings/{run_id}_peerfeedback_fixation.txt\"\n",
    "    feedback_pmod_file = f\"../timings/{run_id}_peerfeedback_feedback_pmod.txt\"\n",
    "    feeling_pmod_file = f\"../timings/{run_id}_peerfeedback_feeling_pmod.txt\"\n",
    "    feedback_pmod_inv_file = f\"../timings/{run_id}_peerfeedback_feedback_pmod_inv.txt\"\n",
    "    feeling_pmod_inv_file = f\"../timings/{run_id}_peerfeedback_feeling_pmod_inv.txt\"\n",
    "    feedback_pmod_exp_file = f\"../timings/{run_id}_peerfeedback_feedback_pmod_exp.txt\"\n",
    "    feeling_pmod_exp_file = f\"../timings/{run_id}_peerfeedback_feeling_pmod_exp.txt\"\n",
    "    feedback_pmod_exp_inv_file = f\"../timings/{run_id}_peerfeedback_feedback_pmod_exp_inv.txt\"\n",
    "    feeling_pmod_exp_inv_file = f\"../timings/{run_id}_peerfeedback_feeling_pmod_exp_inv.txt\"\n",
    "    value = df.loc[1,'peerfeedback_directions.started'] + 7\n",
    "    print(f'reading {file_path}')\n",
    "    df['peer_stim_rating'] = pd.to_numeric(df['peer_stim_rating'].fillna(0).astype(int))  \n",
    "    with open(fixation_file, 'w')  as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "                timing = row['fixation_cross.started']-value\n",
    "                duration = row['fixation_duration']\n",
    "                file.write(f'{timing} {duration} \\n' )\n",
    "    with open(green_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['border_color'] == 'green':\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(green_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['border_color'] == 'green':\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(yellow_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['border_color'] == 'yellow':\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(orange_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['border_color'] == 'orange':\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(peer_like_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['peer_stim_rating'] > 2:\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(peer_dislike_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['peer_stim_rating'] < 3:\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(good_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['feeling_rep.keys'] == 'g' or row['feeling_rep.keys'] == 'r':\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(bad_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['feeling_rep.keys'] == 'b' or row['feeling_rep.keys'] == 'y':\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(green_like_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['border_color'] == 'green' and row['peer_stim_rating'] > 2:\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(yellow_like_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['border_color'] == 'yellow' and row['peer_stim_rating'] > 2:\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(orange_like_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['border_color'] == 'orange' and row['peer_stim_rating'] > 2:\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(green_dislike_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['border_color'] == 'green' and row['peer_stim_rating'] < 3:\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(yellow_dislike_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['border_color'] == 'yellow' and row['peer_stim_rating'] < 3:\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(orange_dislike_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['border_color'] == 'orange' and row['peer_stim_rating'] < 3:\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(feedback_pmod_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['feeling_rep.keys'] in button_keys:\n",
    "                resp = row['peer_stim_rating']\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing}  8  {resp} \\n\")\n",
    "    with open(feeling_pmod_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['feeling_rep.keys'] in button_keys:\n",
    "                resp = button_keys.get(row['feeling_rep.keys'], None)\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing}  8  {resp} \\n\")\n",
    "    with open(feedback_pmod_inv_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['feeling_rep.keys'] in button_keys:\n",
    "                resp = 5 - row['peer_stim_rating']\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing}  8  {resp} \\n\")\n",
    "    with open(feeling_pmod_inv_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['feeling_rep.keys'] in button_keys:\n",
    "                resp = button_keys_inv.get(row['feeling_rep.keys'], None)\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing}  8  {resp} \\n\")\n",
    "    with open(feedback_pmod_exp_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['feeling_rep.keys'] in button_keys:\n",
    "                resp = np.exp(row['peer_stim_rating'])\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing}  8  {resp} \\n\")\n",
    "    with open(feeling_pmod_exp_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['feeling_rep.keys'] in button_keys:\n",
    "                resp = button_keys_exp.get(row['feeling_rep.keys'], None)\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing}  8  {resp} \\n\")\n",
    "    with open(feedback_pmod_exp_inv_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['feeling_rep.keys'] in button_keys:\n",
    "                resp = np.exp(5 - row['peer_stim_rating'])\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing}  8  {resp} \\n\")\n",
    "    with open(feeling_pmod_exp_inv_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['feeling_rep.keys'] in button_keys:\n",
    "                resp = button_keys_exp_inv.get(row['feeling_rep.keys'], None)\n",
    "                timing = row['peerfeedback_stimuli.started']-value\n",
    "                file.write(f\"{timing}  8  {resp} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e19bb27",
   "metadata": {},
   "source": [
    "### Pulling the conformity timings\n",
    "\n",
    "This functions pulls timings from the conformity task. It's basically the same as the other two functions, but also has a bit of code at the beginning that pulls the \"certainty\" ratings from the self-other task. So for this code to work, you'll need to have also uploaded the logs for that participant's self-other task. \n",
    "\n",
    "#### Notes:\n",
    "This script notes changes in ratings after feedback, with SHOULD but might not always equate to conformity. In the future, it might be a good idea to parse out change that aligned with participant feedback from changes that went *against* participant feedback (for example, when a participant learns that peer ratings are LOWER than theirs and in turn makes their rating *higher* than the original).  \n",
    "\n",
    "Right now this function only pulls timings for the presentation of peer feedback (3 seconds). Depending on the research questions, it might also be important to pull timings for other parts of the process (such as when participants are reminded of their own ratings). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f04436db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conformity_times(file_path, run_id, prefix, suffix):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # This section of code pulls the Task 1 files and adds a column for certainty\n",
    "    selfother_search_pattern = f'../logs/selfother_all/{prefix}_task1*.csv'\n",
    "    selfother_logs = glob.glob(selfother_search_pattern)\n",
    "    selfother_dfs = [pd.read_csv(file) for file in selfother_logs]\n",
    "    if len(selfother_dfs) > 1:\n",
    "         selfother_merged = pd.concat(selfother_dfs, ignore_index = True)\n",
    "         df = df.merge(selfother_merged[['stimuli_file','certainty_box.keys']], on = 'stimuli_file', how = 'left')\n",
    "         df = df.iloc[1:].reset_index(drop=True)\n",
    "    else:\n",
    "         df = df.merge(selfother_dfs[0][['stimuli_file', 'certainty_box.keys']], on='stimuli_file', how='left')\n",
    "    # Now back to our regular programming (lol)\n",
    "    same_file = f\"../timings/{run_id}_conformity_same.txt\"\n",
    "    lower_file = f\"../timings/{run_id}_conformity_lower.txt\"\n",
    "    higher_file = f\"../timings/{run_id}_conformity_higher.txt\"\n",
    "    misaligned_file = f\"../timings/{run_id}_conformity_misaligned.txt\"\n",
    "    change_file = f\"../timings/{run_id}_conformity_change.txt\"\n",
    "    nochange_file = f\"../timings/{run_id}_conformity_nochange.txt\"\n",
    "    same_change_file = f\"../timings/{run_id}_conformity_same_change.txt\"\n",
    "    same_nochange_file = f\"../timings/{run_id}_conformity_same_nochange.txt\"\n",
    "    lower_change_file = f\"../timings/{run_id}_conformity_lower_change.txt\"\n",
    "    lower_nochange_file = f\"../timings/{run_id}_conformity_lower_nochange.txt\"\n",
    "    higher_change_file = f\"../timings/{run_id}_conformity_higher_change.txt\"\n",
    "    higher_nochange_file = f\"../timings/{run_id}_conformity_higher_nochange.txt\"\n",
    "    like_change_file = f\"../timings/{run_id}_conformity_like_change.txt\"\n",
    "    like_nochange_file = f\"../timings/{run_id}_conformity_like_nochange.txt\"\n",
    "    dislike_change_file = f\"../timings/{run_id}_conformity_dislike_change.txt\"\n",
    "    dislike_nochange_file = f\"../timings/{run_id}_conformity_dislike_nochange.txt\"\n",
    "    certain_change_file = f\"../timings/{run_id}_conformity_certain_change.txt\"\n",
    "    certain_nochange_file = f\"../timings/{run_id}_conformity_certain_nochange.txt\"\n",
    "    uncertain_change_file = f\"../timings/{run_id}_conformity_uncertain_change.txt\"\n",
    "    uncertain_nochange_file = f\"../timings/{run_id}_conformity_uncertain_nochange.txt\"\n",
    "    fixation_file = f\"../timings/{run_id}_conformity_fixation.txt\"\n",
    "    value = df.loc[1,'si_directions.started'] + 7\n",
    "    print(f'reading {file_path}')\n",
    "    with open(fixation_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "                timing = row['fixation_cross.started']-value\n",
    "                duration = row['fixation_duration']\n",
    "                file.write(f'{timing} {duration} \\n' )\n",
    "    with open(same_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['peer_rating'] == 'SAME':\n",
    "                timing = row['peer_rate.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(lower_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['peer_rating'] == 'LOWER':\n",
    "                timing = row['peer_rate.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(higher_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['peer_rating'] == 'LOWER':\n",
    "                timing = row['peer_rate.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(misaligned_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] != 'None' and (row['peer_rating'] == 'LOWER' or row['peer_rating'] == 'HIGHER'):\n",
    "                timing = row['peer_rate.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(change_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] != 'None' and row['sm_resp'] != row['rerate_box.keys']:\n",
    "                timing = row['peer_rate.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(nochange_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] != 'None' and row['sm_resp'] == row['rerate_box.keys']:\n",
    "                timing = row['peer_rate.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(same_change_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] != 'None' and row['sm_resp'] != row['rerate_box.keys'] and row['peer_rating'] == 'SAME':\n",
    "                timing = row['peer_rate.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(same_nochange_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] != 'None' and row['sm_resp'] == row['rerate_box.keys'] and row['peer_rating'] == 'SAME':\n",
    "                timing = row['peer_rate.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(lower_change_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] != 'None' and row['sm_resp'] != row['rerate_box.keys'] and row['peer_rating'] == 'LOWER':\n",
    "                timing = row['peer_rate.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(lower_nochange_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] != 'None' and row['sm_resp'] == row['rerate_box.keys'] and row['peer_rating'] == 'LOWER':\n",
    "                timing = row['peer_rate.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(higher_change_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] != 'None' and row['sm_resp'] != row['rerate_box.keys'] and row['peer_rating'] == 'HIGHER':\n",
    "                timing = row['peer_rate.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(higher_nochange_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] != 'None' and row['sm_resp'] == row['rerate_box.keys'] and row['peer_rating'] == 'HIGHER':\n",
    "                timing = row['peer_rate.started']-value\n",
    "                file.write(f\"{timing} \\n\")\n",
    "    with open(like_change_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] in button_keys:\n",
    "                resp = button_keys_inv.get(row['sm_resp'], None)\n",
    "                if resp > 2 and row['sm_resp'] == row['rerate_box.keys']:\n",
    "                   timing = row['peer_rate.started']-value\n",
    "                   file.write(f\"{timing} \\n\")\n",
    "    with open(like_nochange_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] in button_keys:\n",
    "                resp = button_keys_inv.get(row['sm_resp'], None)\n",
    "                if resp > 2 and row['sm_resp'] != row['rerate_box.keys']:\n",
    "                   timing = row['peer_rate.started']-value\n",
    "                   file.write(f\"{timing} \\n\")\n",
    "    with open(dislike_change_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] in button_keys:\n",
    "                resp = button_keys_inv.get(row['sm_resp'], None)\n",
    "                if resp < 3 and row['sm_resp'] == row['rerate_box.keys']:\n",
    "                   timing = row['peer_rate.started']-value\n",
    "                   file.write(f\"{timing} \\n\")\n",
    "    with open(dislike_nochange_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] in button_keys:\n",
    "                resp = button_keys_inv.get(row['sm_resp'], None)\n",
    "                if resp < 3 and row['sm_resp'] != row['rerate_box.keys']:\n",
    "                   timing = row['peer_rate.started']-value\n",
    "                   file.write(f\"{timing} \\n\")\n",
    "    with open(certain_change_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] in button_keys and row['certainty_box.keys'] in button_keys:\n",
    "                resp = button_keys_inv.get(row['certainty_box.keys'], None)\n",
    "                if resp > 2 and row['sm_resp'] == row['rerate_box.keys']:\n",
    "                   timing = row['peer_rate.started']-value\n",
    "                   file.write(f\"{timing} \\n\")\n",
    "    with open(certain_nochange_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] in button_keys and row['certainty_box.keys'] in button_keys:\n",
    "                resp = button_keys_inv.get(row['certainty_box.keys'], None)\n",
    "                if resp > 2 and row['sm_resp'] != row['rerate_box.keys']:\n",
    "                   timing = row['peer_rate.started']-value\n",
    "                   file.write(f\"{timing} \\n\")\n",
    "    with open(uncertain_change_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] in button_keys and row['certainty_box.keys'] in button_keys:\n",
    "                resp = button_keys_inv.get(row['certainty_box.keys'], None)\n",
    "                if resp < 3 and row['sm_resp'] == row['rerate_box.keys']:\n",
    "                   timing = row['peer_rate.started']-value\n",
    "                   file.write(f\"{timing} \\n\")\n",
    "    with open(uncertain_nochange_file, 'w') as file:\n",
    "        for index, row in df.iloc[1:].iterrows():\n",
    "            if row['sm_resp'] in button_keys and row['certainty_box.keys'] in button_keys:\n",
    "                resp = button_keys_inv.get(row['certainty_box.keys'], None)\n",
    "                if resp < 3 and row['sm_resp'] != row['rerate_box.keys']:\n",
    "                   timing = row['peer_rate.started']-value\n",
    "                   file.write(f\"{timing} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1dd7f7",
   "metadata": {},
   "source": [
    "### Pulling the Cyberball timings. \n",
    "\n",
    "This function hasn't been built yet, so if you try to pull these timings it will nag you to write the code.\n",
    "\n",
    "#### Note: \n",
    "The tough thing about CyberBall is that it doesn't export a .csv log like the other tasks, but instead just a raw .txt log. I've held off on coding a timing puller for it because I suspect that someone in one of our Co-I's labs has a tool for this already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a58efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyberball_times(file_path, run_id):\n",
    "    print('Please code this function :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b1a302",
   "metadata": {},
   "source": [
    "### The one that binds them \n",
    "\n",
    "This function is pretty simple. Depending on the task you choose (the script will ask you to type it in), it will run the appropriate function over all of the .csvs in the appropriate log directory. We could just have this script do everything for all of the tasks at once, but I guess I was having fun with functions when I coded this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a204244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_csvs(directory,task):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            parts = filename.split('_')\n",
    "            prefix = parts[0]\n",
    "            suffix = parts[-1].replace('.csv','')\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            run_id = f\"{prefix}{suffix}\"\n",
    "            if task == 'self-other':\n",
    "               selfother_times(file_path, run_id)\n",
    "            if task == 'feedback':\n",
    "               peerfeedback_times(file_path, run_id)\n",
    "            if task == 'conformity':\n",
    "               conformity_times(file_path, run_id, prefix,suffix)\n",
    "            if task == 'cyberball':\n",
    "               cyberball_times(file_path, run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d7268",
   "metadata": {},
   "source": [
    "### The interactive part\n",
    "\n",
    "This last part of the script just serves up an input on the screen. It'll ask you what task you want timings for, and then you'll type in the correct answer. Don't add a space or anything to the end of your answer. \n",
    "\n",
    "#### Note:\n",
    "If you end up changing the structure or location of the logs for any reason, you'll need to change this code to reflect that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b916cfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What task would you like timings for? (self-other, feedback, conform, cyberball) feedback\n",
      "reading ../logs/peerfeedback_all/008_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/008_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/038_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/038_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/055_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/055_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/058_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/058_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/079_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/079_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/094_task2_ab.csv\n",
      "reading ../logs/peerfeedback_all/104_task2_ab.csv\n",
      "reading ../logs/peerfeedback_all/110_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/110_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/135_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/135_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/142_task2_ab.csv\n",
      "reading ../logs/peerfeedback_all/143_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/143_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/145_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/145_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/148_task2_ab.csv\n",
      "reading ../logs/peerfeedback_all/177_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/177_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/181_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/181_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/186_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/186_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/194_task2_ab.csv\n",
      "reading ../logs/peerfeedback_all/195_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/195_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/250_task2_ab.csv\n",
      "reading ../logs/peerfeedback_all/262_task2_ab.csv\n",
      "reading ../logs/peerfeedback_all/264_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/264_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/342_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/342_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/343_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/343_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/353_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/353_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/408_task2_ab.csv\n",
      "reading ../logs/peerfeedback_all/416_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/416_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/419_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/419_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/420_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/420_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/422_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/422_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/430_task2_ab.csv\n",
      "reading ../logs/peerfeedback_all/439_task2_ab.csv\n",
      "reading ../logs/peerfeedback_all/443_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/443_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/445_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/445_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/447_task2_ab.csv\n",
      "reading ../logs/peerfeedback_all/456_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/456_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/462_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/462_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/475_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/475_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/476_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/476_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/489_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/489_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/490_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/490_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/508_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/508_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/510_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/510_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/514_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/514_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/544_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/544_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/580_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/580_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/581_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/581_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/583_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/583_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/585_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/585_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/586_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/586_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/588_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/588_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/590_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/590_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/594_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/594_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/595_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/595_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/600_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/600_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/604_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/604_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/621_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/621_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/629_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/629_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/636_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/636_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/637_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/637_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/638_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/638_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/648_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/648_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/660_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/660_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/669_task2_a.csv\n",
      "reading ../logs/peerfeedback_all/669_task2_b.csv\n",
      "reading ../logs/peerfeedback_all/027_task2_ab.csv\n"
     ]
    }
   ],
   "source": [
    "task = input('What task would you like timings for? (self-other, feedback, conform, cyberball) ')\n",
    "\n",
    "if task == 'self-other':\n",
    "    directory = '../logs/selfother_all'\n",
    "if task == 'feedback':\n",
    "    directory = '../logs/peerfeedback_all'\n",
    "if task == 'conform':\n",
    "    directory = '../logs/conformity_all'\n",
    "if task == 'cyberball':\n",
    "    directory = '../logs/cyberball'\n",
    "\n",
    "process_all_csvs(directory,task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4190309",
   "metadata": {},
   "source": [
    "## Step 3: Formatting the timings\n",
    "\n",
    "All of this work has gotten us a whole lot of .txt files, which contain lists of timings for each condition (and in the case of pmod timings, also the duration and hypothesized amplitude modulation). Before we can use these in a regression, though, we need to convert them to the .1D format preferred by AFNI. This script uses an AFNI python tool to:\n",
    "1. Merge timing files together into one (if needed)\n",
    "2. Shift the timing files from long-format .txt files to wide-format .1D files\n",
    "3. \"Marry\" the modulation data with the timing if it exists\n",
    "4. Save the new files in a specific directory depending on whether they are one-run or two-run files. \n",
    "\n",
    "This script is long, repetitive, and changes often. For those reasons, I've just included a really short snippet that should give you some idea of what's going on. To see it all, and to run it, check out B3/scripts/step_three_b_formattingtimings.sh. \n",
    "\n",
    "A few notes about it:\n",
    "* Right now it's designed to format ALL of the files that can be pulled for ALL tasks. If you only want to do this for one specific task, you'll need to comment out all of the other lines by adding \"#\" to the start of those lines. \n",
    "\n",
    "\n",
    "* If you want to create new files in the previous step, you'll need to add lines for them here. Remember to add both two-run (the first \"if\" loop) and one-run (the second \"if\" loop) options.\n",
    "\n",
    "\n",
    "* Right now this pulls from all subjects listed in the file subjList.txt. To change the subjects, either edit this file (nano subjList.txt) or just replace \"\\`cat subjList.txt\\`\" with the subject IDs. (Note that this includes removing the \\` symbols).\n",
    "\n",
    "\n",
    "* The \"file1\" and \"file2\" at the top are used to determine if the subject has one or two runs. If a subject has this particular file coded for one or two runs, it will treat ALL files for the subject that way. Thus, any participant who has two runs of MOST tasks but one run for a single task (due to an error during the scan) will need some special treatment. If you run this script in the terminal, you'll see an error for that participant. I recommend building a custom script for them or just running the correct commands directly in the command line. \n",
    "\n",
    "\n",
    "* For each participant, this script will tell you the participant is NOT a one-run or two-run participant. If the script says BOTH of these things for any given participant, there is something wrong with their files.\n",
    "\n",
    "#### Note:\n",
    "This is DEFINITELY a way to make this code far shorter and more efficient by calling in the filenames from an external list (just as we can do for subjects). But I haven't done that yet --- if someone DOES decide to do that, just be mindful of the fact that some of these timings have one file while others have two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eebfb72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "008  is not a two-run participant\n",
      "008  is not a one-run participant\n",
      "038  is not a two-run participant\n",
      "038  is not a one-run participant\n",
      "055  is not a two-run participant\n",
      "055  is not a one-run participant\n",
      "058  is not a two-run participant\n",
      "058  is not a one-run participant\n",
      "079  is not a two-run participant\n",
      "079  is not a one-run participant\n",
      "094  is not a two-run participant\n",
      "094  is not a one-run participant\n",
      "104  is not a two-run participant\n",
      "104  is not a one-run participant\n",
      "110  is not a two-run participant\n",
      "110  is not a one-run participant\n",
      "127  is not a two-run participant\n",
      "127  is not a one-run participant\n",
      "135  is not a two-run participant\n",
      "135  is not a one-run participant\n",
      "142  is not a two-run participant\n",
      "142  is not a one-run participant\n",
      "143  is not a two-run participant\n",
      "143  is not a one-run participant\n",
      "145  is not a two-run participant\n",
      "145  is not a one-run participant\n",
      "148  is not a two-run participant\n",
      "148  is not a one-run participant\n",
      "177  is not a two-run participant\n",
      "177  is not a one-run participant\n",
      "181  is not a two-run participant\n",
      "181  is not a one-run participant\n",
      "194  is not a two-run participant\n",
      "194  is not a one-run participant\n",
      "250  is not a two-run participant\n",
      "250  is not a one-run participant\n",
      "342  is not a two-run participant\n",
      "342  is not a one-run participant\n",
      "353  is not a two-run participant\n",
      "353  is not a one-run participant\n",
      "416  is not a two-run participant\n",
      "416  is not a one-run participant\n",
      "419  is not a two-run participant\n",
      "419  is not a one-run participant\n",
      "420  is not a two-run participant\n",
      "420  is not a one-run participant\n",
      "422  is not a two-run participant\n",
      "422  is not a one-run participant\n",
      "443  is not a two-run participant\n",
      "443  is not a one-run participant\n",
      "445  is not a two-run participant\n",
      "445  is not a one-run participant\n",
      "447  is not a two-run participant\n",
      "447  is not a one-run participant\n",
      "456  is not a two-run participant\n",
      "456  is not a one-run participant\n",
      "462  is not a two-run participant\n",
      "462  is not a one-run participant\n",
      "475  is not a two-run participant\n",
      "475  is not a one-run participant\n",
      "476  is not a two-run participant\n",
      "476  is not a one-run participant\n",
      "489  is not a two-run participant\n",
      "489  is not a one-run participant\n",
      "490  is not a two-run participant\n",
      "490  is not a one-run participant\n",
      "508  is not a two-run participant\n",
      "508  is not a one-run participant\n",
      "580  is not a two-run participant\n",
      "580  is not a one-run participant\n",
      "581  is not a two-run participant\n",
      "581  is not a one-run participant\n",
      "583  is not a two-run participant\n",
      "583  is not a one-run participant\n",
      "585  is not a two-run participant\n",
      "585  is not a one-run participant\n",
      "586  is not a two-run participant\n",
      "586  is not a one-run participant\n",
      "588  is not a two-run participant\n",
      "588  is not a one-run participant\n",
      "590  is not a two-run participant\n",
      "590  is not a one-run participant\n",
      "594  is not a two-run participant\n",
      "594  is not a one-run participant\n",
      "595  is not a two-run participant\n",
      "595  is not a one-run participant\n",
      "600  is not a two-run participant\n",
      "600  is not a one-run participant\n",
      "604  is not a two-run participant\n",
      "604  is not a one-run participant\n",
      "621  is not a two-run participant\n",
      "621  is not a one-run participant\n",
      "629  is not a two-run participant\n",
      "629  is not a one-run participant\n",
      "636  is not a two-run participant\n",
      "636  is not a one-run participant\n",
      "637  is not a two-run participant\n",
      "637  is not a one-run participant\n",
      "638  is not a two-run participant\n",
      "638  is not a one-run participant\n",
      "648  is not a two-run participant\n",
      "648  is not a one-run participant\n",
      "660  is not a two-run participant\n",
      "660  is not a one-run participant\n",
      "669  is not a two-run participant\n",
      "669  is not a one-run participant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n",
      "bash: line 4: file1: command not found\n",
      "bash: line 5: file2: command not found\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "for subj in `cat subjList.txt`; do\n",
    "\n",
    "file1 = \"../timings/${subj}a_peerfeedback_bad.txt\"\n",
    "file2 = \"../timings/${subj}ab_peerfeedback_bad.txt\"\n",
    "\n",
    "if [ -e \"${file1}\" ]; then\n",
    "\n",
    "  timing_tool.py -fsl_timing_files \"../timings/${subj}a_selfother_other.txt\" \"../timings/${subj}b_selfother_other.txt\" -write_timing \"../timings/tworun1D/${subj}.selfother_other.1D\"\n",
    "  timing_tool.py -fsl_timing_files \"../timings/${subj}a_peerfeedback_bad.txt\" \"../timings/${subj}b_peerfeedback_bad.txt\" -write_timing \"../timings/tworun1D/${subj}.peerfeedback_bad.1D\"\n",
    "  timing_tool.py -fsl_timing_files \"../timings/${subj}a_conformity_same.txt\" \"../timings/${subj}b_conformity_same.txt\" -write_timing \"../timings/tworun1D/${subj}.conformity_same.1D\"\n",
    "\n",
    "else\n",
    "echo ${subj} \" is not a two-run participant\"\n",
    "\n",
    "fi\n",
    "\n",
    "if [ -e \"${file2}\" ]; then\n",
    "\n",
    "  timing_tool.py -fsl_timing_files \"../timings/${subj}ab_selfother_other.txt\" -write_timing \"../timings/onerun1D/${subj}.selfother_other.1D\"\n",
    "  timing_tool.py -fsl_timing_files \"../timings/${subj}ab_peerfeedback_bad.txt\" -write_timing \"../timings/onerun1D/${subj}.peerfeedback_bad.1D\"\n",
    "  timing_tool.py -fsl_timing_files \"../timings/${subj}ab_conformity_same.txt\" -write_timing \"../timings/onerun1D/${subj}.conformity_same.1D\"\n",
    "\n",
    "else \n",
    "echo ${subj} \" is not a one-run participant\"\n",
    "\n",
    "fi \n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734ef7f4",
   "metadata": {},
   "source": [
    "## That's it! \n",
    "\n",
    "Congrats! You've just pulled all of the timing files you'll need for your analysis. As a next step, check out the STEP_4_First_Level_Analysis notebook in the directory. \n",
    "\n",
    "It's also generally a good practice to actually look at a few of the .1D timing files you've produced to make sure they align with the timings you'd expect given that participant's logs. Remember that all of the numbers will be smaller than the timing numbers in the logs by a value that varies across runs (but is the same for every timing in each single run).\n",
    "\n",
    "\n",
    "Thanks and cheers,\n",
    "\n",
    "Matt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
